{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\uddf1 Build a Sales Data Mart (SQL Example)\n", "\n", "This notebook demonstrates how to build a simple sales data mart using SQL logic in PySpark."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install dependencies (if needed)\n", "!pip install pyspark delta-spark"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Initialize Spark with Delta support\n", "from pyspark.sql import SparkSession\n", "from delta import configure_spark_with_delta_pip\n", "\n", "builder = SparkSession.builder \\\n", "    .appName(\"Sales Data Mart\") \\\n", "    .master(\"local[*]\") \\\n", "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n", "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n", "\n", "spark = configure_spark_with_delta_pip(builder).getOrCreate()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\uddfe Create Raw DataFrames (Simulating a Data Warehouse)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["orders = [(1, '2023-01-01', 101, 100.0, 5.0, 'store_1', 'emp_1'),\n", "          (2, '2023-01-02', 102, 200.0, 10.0, 'store_2', 'emp_2')]\n", "products = [(101, 'Shoes', 'Footwear'),\n", "            (102, 'T-shirt', 'Apparel')]\n", "\n", "df_orders = spark.createDataFrame(orders, [\"order_id\", \"order_date\", \"product_id\", \"sales_amount\", \"discount\", \"store_id\", \"salesperson_id\"])\n", "df_products = spark.createDataFrame(products, [\"product_id\", \"product_name\", \"category\"])\n", "\n", "df_orders.createOrReplaceTempView(\"raw_orders\")\n", "df_products.createOrReplaceTempView(\"raw_products\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\uddf1 Create the Sales Data Mart"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sales_mart = spark.sql(\"\"\"\n", "    SELECT\n", "        o.order_id,\n", "        o.order_date,\n", "        p.product_name,\n", "        p.category,\n", "        o.sales_amount,\n", "        o.discount,\n", "        o.store_id,\n", "        o.salesperson_id\n", "    FROM raw_orders o\n", "    JOIN raw_products p ON o.product_id = p.product_id\n", "\"\"\")\n", "\n", "sales_mart.show()\n", "\n", "sales_mart.write.format(\"delta\").mode(\"overwrite\").save(\"file:///C:/data-lake/sales_mart\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcca Query the Data Mart"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sales_mart.createOrReplaceTempView(\"sales_mart\")\n", "spark.sql(\"\"\"\n", "    SELECT category, SUM(sales_amount) AS total_revenue\n", "    FROM sales_mart\n", "    GROUP BY category\n", "\"\"\").show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cleanup\n", "spark.stop()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 5}